{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df6152a-2284-4368-a639-5e400baffd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building a Persistent Memory System for AI Agents with Databricks Lakebase\n",
    "\n",
    "In this tutorial, we'll build a persistent memory system for AI agents using **Databricks Lakebase Provisioned** - a fully managed PostgreSQL database integrated with the Databricks platform.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A conversation memory system that stores:\n",
    "* User messages and agent responses\n",
    "* Session tracking\n",
    "* Timestamps\n",
    "* Metadata (intents, entities, etc.)\n",
    "\n",
    "## Why Lakebase?\n",
    "\n",
    "* **Fully Managed PostgreSQL**: No infrastructure management\n",
    "* **Databricks Integration**: Seamless authentication and governance\n",
    "* **ACID Transactions**: Perfect for conversational state management\n",
    "* **Scalable**: Start small (1 CU) and scale as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Databricks workspace (AWS)\n",
    "* Python environment with databricks-sdk\n",
    "* Basic familiarity with PostgreSQL\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7297e8-92e9-43f8-9732-06b079f8b75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 0: Install Dependencies\n",
    "\n",
    "First, we'll install the required packages:\n",
    "* `databricks-sdk`: For Lakebase API access\n",
    "* `psycopg2-binary`: PostgreSQL driver for Python\n",
    "* Additional packages for building AI agents (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6dc88d-294f-4ebe-b3f9-0022573e3c55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 0: Install Dependencies"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sdk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346/lib/python3.12/site-packages (0.78.0)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk) (2.32.3)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk) (2.40.0)\nRequirement already satisfied: protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346/lib/python3.12/site-packages (from databricks-sdk) (6.33.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (5.5.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (4.9.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2025.1.31)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: psycopg2-binary in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346/lib/python3.12/site-packages (2.9.11)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting mlflow\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting langgraph\n  Downloading langgraph-1.0.6-py3-none-any.whl.metadata (7.4 kB)\nCollecting langchain-openai\n  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\nCollecting databricks-langchain\n  Downloading databricks_langchain-0.13.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting langgraph-checkpoint-postgres\n  Downloading langgraph_checkpoint_postgres-3.0.3-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: pydantic in /databricks/python3/lib/python3.12/site-packages (2.10.6)\nCollecting psycopg[binary]\n  Downloading psycopg-3.3.2-py3-none-any.whl.metadata (4.3 kB)\nCollecting mlflow-skinny==3.8.1 (from mlflow)\n  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.8.1 (from mlflow)\n  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\nCollecting Flask-CORS<7 (from mlflow)\n  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\nCollecting Flask<4 (from mlflow)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow)\n  Downloading alembic-1.18.1-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (43.0.3)\nCollecting docker<8,>=4.0.0 (from mlflow)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting huey<3,>=2.5.0 (from mlflow)\n  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.10.0)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.1.3)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.15.1)\nCollecting sqlalchemy<3,>=1.4.0 (from mlflow)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.78.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (24.1)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.33.4)\nCollecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (4.12.2)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.34.2)\nCollecting langchain-core>=0.1 (from langgraph)\n  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.6-py3-none-any.whl.metadata (5.2 kB)\nCollecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n  Downloading langgraph_sdk-0.3.3-py3-none-any.whl.metadata (1.6 kB)\nCollecting xxhash>=3.5.0 (from langgraph)\n  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting openai<3.0.0,>=1.109.1 (from langchain-openai)\n  Downloading openai-2.15.0-py3-none-any.whl.metadata (29 kB)\nCollecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\nCollecting databricks-ai-bridge>=0.4.2 (from databricks-langchain)\n  Downloading databricks_ai_bridge-0.12.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting databricks-mcp>=0.5.1 (from databricks-langchain)\n  Downloading databricks_mcp-0.6.0-py3-none-any.whl.metadata (1.1 kB)\nCollecting databricks-vectorsearch>=0.50 (from databricks-langchain)\n  Downloading databricks_vectorsearch-0.63-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain-mcp-adapters>=0.1.13 (from databricks-langchain)\n  Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl.metadata (10 kB)\nCollecting langchain>=1.0.0 (from databricks-langchain)\n  Downloading langchain-1.2.4-py3-none-any.whl.metadata (4.9 kB)\nCollecting unitycatalog-langchain>=0.3.0 (from unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_langchain-0.3.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting orjson>=3.10.1 (from langgraph-checkpoint-postgres)\n  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\nCollecting psycopg-pool>=3.2.0 (from langgraph-checkpoint-postgres)\n  Downloading psycopg_pool-3.3.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting psycopg-binary==3.3.2 (from psycopg[binary])\n  Downloading psycopg_binary-3.3.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic) (2.27.2)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow) (1.17.1)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting mcp>=1.13.0 (from databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading mcp-1.25.0-py3-none-any.whl.metadata (89 kB)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.40.0)\nCollecting deprecation>=2 (from databricks-vectorsearch>=0.50->databricks-langchain)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: jinja2>=3.1.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.5)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.2)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nCollecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core>=0.1->langgraph)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langsmith<1.0.0,>=0.3.45 (from langchain-core>=0.1->langgraph)\n  Downloading langsmith-0.6.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (9.0.0)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core>=0.1->langgraph)\n  Downloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_x86_64.whl.metadata (5.4 kB)\nCollecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph)\n  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx>=0.25.2 in /databricks/python3/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.27.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.6.2)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\nCollecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\nCollecting tqdm>4 (from openai<3.0.0,>=1.109.1->langchain-openai)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nCollecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain-openai)\n  Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting langchain-community>=0.4.0 (from unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_ai-0.3.2-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.7)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.21)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.11)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.14.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.21.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting requests<3,>=2.17.3 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nCollecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\nCollecting httpx>=0.25.2 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: jsonschema>=4.20.0 in /databricks/python3/lib/python3.12/site-packages (from mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain) (4.23.0)\nCollecting pydantic\n  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\nRequirement already satisfied: pyjwt>=2.10.1 in /databricks/python3/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain) (2.10.1)\nCollecting python-multipart>=0.0.9 (from mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading sse_starlette-3.1.2-py3-none-any.whl.metadata (12 kB)\nCollecting typing-inspection>=0.4.1 (from mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting pydantic-core==2.41.5 (from pydantic)\n  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.53b1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.3.2)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.6.0)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_client-0.3.1-py3-none-any.whl.metadata (7.8 kB)\nCollecting databricks-connect<17.1,>=15.1.0 (from unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading databricks_connect-17.0.10-py2.py3-none-any.whl.metadata (2.6 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (24.3.0)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\nRequirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: grpcio>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.10.9.9)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (74.0.0)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain) (0.22.3)\nCollecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.8)\nINFO: pip is looking at multiple versions of sse-starlette to determine which version is compatible with other requirements. This could take a while.\nCollecting sse-starlette>=1.6.1 (from mcp>=1.13.0->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading sse_starlette-3.1.1-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.1.0-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.0.4-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.0.3-py3-none-any.whl.metadata (12 kB)\nCollecting anyio<5,>=3.5.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nCollecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.4.0->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.0.0)\nDownloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.1/9.1 MB\u001B[0m \u001B[31m156.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m124.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m77.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langgraph-1.0.6-py3-none-any.whl (157 kB)\nDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\nDownloading databricks_langchain-0.13.0-py3-none-any.whl (36 kB)\nDownloading langgraph_checkpoint_postgres-3.0.3-py3-none-any.whl (42 kB)\nDownloading psycopg_binary-3.3.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/5.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m150.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading alembic-1.18.1-py3-none-any.whl (260 kB)\nDownloading databricks_ai_bridge-0.12.0-py3-none-any.whl (24 kB)\nDownloading databricks_mcp-0.6.0-py3-none-any.whl (12 kB)\nDownloading databricks_vectorsearch-0.63-py3-none-any.whl (19 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading huey-2.6.0-py3-none-any.whl (76 kB)\nDownloading langchain-1.2.4-py3-none-any.whl (107 kB)\nDownloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\nDownloading langchain_mcp_adapters-0.2.1-py3-none-any.whl (22 kB)\nDownloading langgraph_checkpoint-4.0.0-py3-none-any.whl (46 kB)\nDownloading langgraph_prebuilt-1.0.6-py3-none-any.whl (35 kB)\nDownloading langgraph_sdk-0.3.3-py3-none-any.whl (67 kB)\nDownloading openai-2.15.0-py3-none-any.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m52.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\nDownloading psycopg-3.3.2-py3-none-any.whl (212 kB)\nDownloading psycopg_pool-3.3.0-py3-none-any.whl (39 kB)\nDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m118.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m55.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading unitycatalog_langchain-0.3.0-py3-none-any.whl (5.3 kB)\nDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/609.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m609.9/609.9 kB\u001B[0m \u001B[31m24.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m101.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langsmith-0.6.4-py3-none-any.whl (283 kB)\nDownloading mcp-1.25.0-py3-none-any.whl (233 kB)\nDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\nDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m104.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\nDownloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/803.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m803.6/803.6 kB\u001B[0m \u001B[31m39.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_x86_64.whl (342 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading unitycatalog_ai-0.3.2-py3-none-any.whl (66 kB)\nDownloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m90.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_connect-17.0.10-py2.py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m114.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m50.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading python_multipart-0.0.21-py3-none-any.whl (24 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading sse_starlette-3.0.3-py3-none-any.whl (11 kB)\nDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\nDownloading unitycatalog_client-0.3.1-py3-none-any.whl (176 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\nDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\nDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\nDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\nInstalling collected packages: huey, xxhash, werkzeug, uuid-utils, typing-extensions, tqdm, tabulate, requests, regex, python-multipart, python-dotenv, psycopg-binary, protobuf, propcache, ormsgpack, orjson, multidict, marshmallow, Mako, jsonpatch, jiter, itsdangerous, httpx-sse, gunicorn, greenlet, graphql-core, frozenlist, deprecation, blinker, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, tiktoken, sqlalchemy, requests-toolbelt, pydantic-core, psycopg-pool, psycopg, opentelemetry-proto, graphql-relay, Flask, docker, anyio, aiosignal, sse-starlette, pydantic, httpx, graphene, Flask-CORS, dataclasses-json, alembic, aiohttp, pydantic-settings, openai, langsmith, langgraph-sdk, databricks-connect, aiohttp-retry, unitycatalog-client, mlflow-tracing, mlflow-skinny, mcp, langchain-core, unitycatalog-ai, mlflow, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-mcp-adapters, databricks-vectorsearch, langgraph-prebuilt, langgraph-checkpoint-postgres, langchain-classic, databricks-ai-bridge, langgraph, langchain-community, databricks-mcp, langchain, unitycatalog-langchain, databricks-langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.4\n    Uninstalling protobuf-6.33.4:\n      Successfully uninstalled protobuf-6.33.4\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.27.2\n    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.6.2\n    Not uninstalling anyio at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'anyio'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Not uninstalling httpx at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'httpx'. No files were found to uninstall.\n  Attempting uninstall: databricks-connect\n    Found existing installation: databricks-connect 17.2.4\n    Not uninstalling databricks-connect at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'databricks-connect'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.0\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bcf9e895-e76b-4235-97f5-71930264f346\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.18.1 anyio-4.12.1 blinker-1.9.0 databricks-ai-bridge-0.12.0 databricks-connect-17.0.10 databricks-langchain-0.13.0 databricks-mcp-0.6.0 databricks-vectorsearch-0.63 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 httpx-0.28.1 httpx-sse-0.4.3 huey-2.6.0 itsdangerous-2.2.0 jiter-0.12.0 jsonpatch-1.33 langchain-1.2.4 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.7 langchain-mcp-adapters-0.2.1 langchain-openai-1.1.7 langchain-text-splitters-1.1.0 langgraph-1.0.6 langgraph-checkpoint-4.0.0 langgraph-checkpoint-postgres-3.0.3 langgraph-prebuilt-1.0.6 langgraph-sdk-0.3.3 langsmith-0.6.4 marshmallow-3.26.2 mcp-1.25.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 multidict-6.7.0 openai-2.15.0 opentelemetry-proto-1.39.1 orjson-3.11.5 ormsgpack-1.12.1 propcache-0.4.1 protobuf-5.29.5 psycopg-3.3.2 psycopg-binary-3.3.2 psycopg-pool-3.3.0 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 python-multipart-0.0.21 regex-2026.1.15 requests-2.32.5 requests-toolbelt-1.0.0 sqlalchemy-2.0.45 sse-starlette-3.0.3 tabulate-0.9.0 tiktoken-0.12.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 unitycatalog-ai-0.3.2 unitycatalog-client-0.3.1 unitycatalog-langchain-0.3.0 uuid-utils-0.13.0 werkzeug-3.1.5 xxhash-3.6.0 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade databricks-sdk \n",
    "%pip install psycopg2-binary\n",
    "%pip install mlflow langgraph langchain-openai databricks-langchain langgraph-checkpoint-postgres psycopg[binary] pydantic\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "527007ce-1219-4959-ad44-412f5e79a2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create a Lakebase Instance\n",
    "\n",
    "A **Lakebase instance** is a managed PostgreSQL server. We'll:\n",
    "1. Initialize the Databricks SDK\n",
    "2. Create an instance with 1 Compute Unit (CU)\n",
    "3. Handle the case where the instance already exists\n",
    "\n",
    "The instance takes a few minutes to provision and will be in `STARTING` or `AVAILABLE` state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6dffdf-5ef3-436d-aaf1-7785d36ff128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create a Custom Database\n",
    "\n",
    "Now we'll:\n",
    "1. Generate OAuth credentials for the instance\n",
    "2. Connect using psycopg2\n",
    "3. Create a custom database called `travel_assistant_db`\n",
    "\n",
    "**Note**: We use OAuth token authentication with your Databricks identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52fa1986-da56-4538-b6d2-58f693cca14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Create the Conversation Memory Table\n",
    "\n",
    "We'll create a table with:\n",
    "* `id`: Auto-incrementing primary key\n",
    "* `session_id`: Track conversation sessions\n",
    "* `user_message` & `agent_response`: Store the conversation\n",
    "* `timestamp`: When the interaction occurred\n",
    "* `metadata`: JSONB field for flexible data (intents, entities, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa67ebe-29d5-41e4-a121-86a51b7faf48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Query the Conversation History\n",
    "\n",
    "Let's verify everything works by querying the data we just inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2250996b-1c75-4713-bf38-c1a0c992ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Create a Reusable Connection Helper\n",
    "\n",
    "For production use, we'll create a helper function that:\n",
    "* Generates fresh credentials automatically\n",
    "* Returns a ready-to-use connection\n",
    "* Can be called from anywhere in your code\n",
    "\n",
    "This is useful for AI agent applications that need to persist state across conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "553e9a70-fa30-4ef0-ad59-76ceadf2f975",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Create Lakebase Instance"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lakebase instance already exists: travel-agent-memory\nConnection endpoint: instance-756cf329-0f27-4995-9dff-2b6bb01d3a29.database.cloud.databricks.com\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.database import DatabaseInstance\n",
    "from databricks.sdk.errors import InvalidParameterValue\n",
    "\n",
    "# Initialize the workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Configuration\n",
    "LAKEBASE_INSTANCE_NAME = \"travel-agent-memory\"\n",
    "\n",
    "try:\n",
    "    # Try to create the Lakebase instance\n",
    "    instance = w.database.create_database_instance(\n",
    "        DatabaseInstance(\n",
    "            name=LAKEBASE_INSTANCE_NAME,\n",
    "            capacity=\"CU_1\"  # Start with 1 Compute Unit\n",
    "        )\n",
    "    )\n",
    "    print(f\"✓ Created Lakebase instance: {instance.name}\")\n",
    "except InvalidParameterValue as e:\n",
    "    if \"not unique\" in str(e):\n",
    "        # Instance already exists, retrieve it\n",
    "        instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "        print(f\"✓ Lakebase instance already exists: {instance.name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Display instance details\n",
    "print(f\"\\nInstance Details:\")\n",
    "print(f\"  Name: {instance.name}\")\n",
    "print(f\"  State: {instance.state}\")\n",
    "print(f\"  Endpoint: {instance.read_write_dns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a888adb-bf3b-437f-8603-8e362174e350",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create Database and Table"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated credentials for: travel-agent-memory\n  Hostname: instance-756cf329-0f27-4995-9dff-2b6bb01d3a29.database.cloud.databricks.com\n  Username: ankit.yadav@databricks.com\n\n✓ Connected to Lakebase instance\n✓ Database already exists: travel_assistant_db\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Helper function to get connection details\n",
    "def get_connection_details():\n",
    "    \"\"\"Get Lakebase connection details with fresh credentials.\"\"\"\n",
    "    credential = w.database.generate_database_credential(\n",
    "        instance_names=[LAKEBASE_INSTANCE_NAME]\n",
    "    )\n",
    "    instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "    current_user = w.current_user.me()\n",
    "    \n",
    "    return {\n",
    "        \"host\": instance.read_write_dns,\n",
    "        \"user\": current_user.user_name,\n",
    "        \"password\": credential.token\n",
    "    }\n",
    "\n",
    "conn_details = get_connection_details()\n",
    "print(f\"✓ Generated credentials for: {conn_details['user']}\")\n",
    "\n",
    "# Connect to the default postgres database\n",
    "conn = psycopg2.connect(\n",
    "    host=conn_details['host'],\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=conn_details['user'],\n",
    "    password=conn_details['password'],\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"✓ Connected to Lakebase instance\")\n",
    "\n",
    "# Create a custom database\n",
    "try:\n",
    "    cursor.execute(\"CREATE DATABASE travel_assistant_db\")\n",
    "    print(f\"✓ Created database: travel_assistant_db\")\n",
    "except psycopg2.errors.DuplicateDatabase:\n",
    "    print(f\"✓ Database already exists: travel_assistant_db\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Connect to the new database and create table\n",
    "conn = psycopg2.connect(\n",
    "    host=conn_details['host'],\n",
    "    port=5432,\n",
    "    database=\"travel_assistant_db\",\n",
    "    user=conn_details['user'],\n",
    "    password=conn_details['password'],\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table for conversation memory\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS conversation_memory (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        session_id VARCHAR(255) NOT NULL,\n",
    "        user_message TEXT,\n",
    "        agent_response TEXT,\n",
    "        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        metadata JSONB\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(f\"✓ Created table: conversation_memory\")\n",
    "\n",
    "# Insert sample conversation\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO conversation_memory (session_id, user_message, agent_response, metadata)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "\"\"\", (\n",
    "    \"demo-session-001\",\n",
    "    \"I want to book a flight to Paris\",\n",
    "    \"I can help you with that. When would you like to travel?\",\n",
    "    '{\"intent\": \"flight_booking\", \"destination\": \"Paris\"}'\n",
    "))\n",
    "conn.commit()\n",
    "print(f\"✓ Inserted sample conversation\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130d26c8-f8b9-48cc-b53f-92b16059e359",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Query the Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent conversations:\n--------------------------------------------------------------------------------\nSession: sample-session-001\nUser: I want to book a flight to Paris\nAgent: I can help you with that. When would you like to travel?\nTime: 2026-01-16 11:01:53.401011\n--------------------------------------------------------------------------------\nSession: sample-session-001\nUser: I want to book a flight to Paris\nAgent: I can help you with that. When would you like to travel?\nTime: 2026-01-16 10:19:40.395614\n--------------------------------------------------------------------------------\n\n✓ Successfully queried 2 records directly from Lakebase\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Get fresh connection details\n",
    "conn_details = get_connection_details()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=conn_details['host'],\n",
    "    port=5432,\n",
    "    database=\"travel_assistant_db\",\n",
    "    user=conn_details['user'],\n",
    "    password=conn_details['password'],\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query conversation history\n",
    "cursor.execute(\"SELECT * FROM conversation_memory ORDER BY timestamp DESC LIMIT 10\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(f\"Recent Conversations ({len(rows)} records):\")\n",
    "print(\"=\" * 80)\n",
    "for row in rows:\n",
    "    print(f\"Session: {row[1]}\")\n",
    "    print(f\"User: {row[2]}\")\n",
    "    print(f\"Agent: {row[3]}\")\n",
    "    print(f\"Time: {row[4]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb2bfb8-e8f1-405d-8a79-5e78b176dd95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Verify Connection Helper"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connection function created successfully\n\nUsage:\n  conn = get_lakebase_connection()\n  cursor = conn.cursor()\n  cursor.execute('SELECT * FROM conversation_memory')\n  rows = cursor.fetchall()\n"
     ]
    }
   ],
   "source": [
    "# Test the connection helper function we created\n",
    "conn = get_lakebase_connection()\n",
    "print(\"✓ Connection helper function works successfully\")\n",
    "\n",
    "# Quick test query\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM conversation_memory\")\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"✓ Can access conversation_memory table ({count} rows)\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n✓ Ready to integrate with LangGraph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88f1c267-234e-4e6d-948e-8116fe8c1058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have a fully functional persistent memory system for AI agents! Here's what you can do next:\n",
    "\n",
    "### 1. Integrate with LangGraph\n",
    "```python\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "conn = get_lakebase_connection()\n",
    "checkpointer = PostgresSaver(conn)\n",
    "# Use with your LangGraph agent\n",
    "```\n",
    "\n",
    "### 2. Add More Tables\n",
    "Create additional tables for:\n",
    "* User preferences\n",
    "* Conversation summaries\n",
    "* Tool call history\n",
    "* Agent performance metrics\n",
    "\n",
    "### 3. Scale Your Instance\n",
    "As your application grows, scale up:\n",
    "```python\n",
    "w.database.update_database_instance(\n",
    "    name=LAKEBASE_INSTANCE_NAME,\n",
    "    database_instance=DatabaseInstance(\n",
    "        name=LAKEBASE_INSTANCE_NAME,\n",
    "        capacity=\"CU_2\"  # Scale to 2 CUs\n",
    "    ),\n",
    "    update_mask=\"capacity\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Register in Unity Catalog (Optional)\n",
    "For data governance and SQL access:\n",
    "```python\n",
    "from databricks.sdk.service.database import DatabaseCatalog\n",
    "\n",
    "w.database.create_database_catalog(\n",
    "    DatabaseCatalog(\n",
    "        name=\"travel_assistant_catalog\",\n",
    "        database_instance_name=LAKEBASE_INSTANCE_NAME,\n",
    "        database_name=\"travel_assistant_db\"\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* **Lakebase Provisioned** provides managed PostgreSQL with Databricks integration\n",
    "* **OAuth authentication** uses your Databricks identity - no password management\n",
    "* **psycopg2** gives you full PostgreSQL capabilities for complex queries\n",
    "* **Perfect for AI agents** that need persistent, transactional state management\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Lakebase Documentation](https://docs.databricks.com/aws/en/oltp/instances/about/)\n",
    "* [Databricks SDK for Python](https://databricks-sdk-py.readthedocs.io/)\n",
    "* [LangGraph Checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "\n",
    "Happy building! \uD83D\uDE80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d44fc82-88dd-4be5-8fba-a0a1e8aabef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Building a Stateful AI Agent with LangGraph\n",
    "\n",
    "Now that we have our Lakebase database set up, let's build a **stateful conversational agent** using LangGraph. This agent will:\n",
    "\n",
    "* **Remember conversations** across multiple turns\n",
    "* **Track structured information** (destination, dates, budget)\n",
    "* **Persist state** in Lakebase using PostgreSQL checkpoints\n",
    "* **Support time-travel** - branch from any point in conversation history\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "LangGraph is a framework for building stateful, multi-actor applications with LLMs. Key features:\n",
    "\n",
    "* **State Management**: Automatically persists conversation state\n",
    "* **Checkpointing**: Save and restore from any point in the conversation\n",
    "* **Graph-based**: Define agent logic as a directed graph of nodes\n",
    "* **Framework Agnostic**: Works with any LLM provider\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Our agent consists of:\n",
    "1. **State Schema**: Defines what information to track\n",
    "2. **Connection Manager**: Handles Lakebase connections with psycopg3\n",
    "3. **Agent Graph**: The LLM logic with checkpointing enabled\n",
    "4. **Conversation Runner**: Executes multi-turn conversations\n",
    "\n",
    "Let's build each component!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53764ebb-9634-4f59-b120-6a3e2b00d2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Define the Agent State Schema\n",
    "\n",
    "The state schema defines what information our agent tracks across conversations. We use TypedDict with LangGraph's `add_messages` annotation to automatically accumulate conversation history.\n",
    "\n",
    "**Key fields:**\n",
    "* `messages`: Conversation history (auto-accumulated by LangGraph)\n",
    "* `destination`, `travel_dates`, `budget`, `trip_type`: Extracted travel preferences\n",
    "* `preferences`: List of user preferences collected during conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b873448-3ad3-4c54-bd0a-4679b1e74f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class TravelPlannerState(TypedDict):\n",
    "    \"\"\"State for the Travel Planning Assistant.\n",
    "    \n",
    "    This state is checkpointed after each interaction, allowing the agent\n",
    "    to maintain context across multiple turns in the conversation.\n",
    "    \"\"\"\n",
    "    # Conversation history - LangGraph handles message accumulation\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # Structured travel preferences extracted from conversation\n",
    "    destination: str | None\n",
    "    travel_dates: str | None\n",
    "    budget: str | None\n",
    "    trip_type: str | None  # e.g., \"relaxation\", \"adventure\", \"cultural\"\n",
    "    \n",
    "    # Collected preferences as a list\n",
    "    preferences: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932032de-e9c8-4298-b27c-8bcf13ae5b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Create a Connection Manager for LangGraph\n",
    "\n",
    "The `LakebaseConnectionManager` provides a clean interface for LangGraph's `PostgresSaver` to connect to our Lakebase database.\n",
    "\n",
    "**Why psycopg3?** LangGraph's PostgreSQL checkpointer requires psycopg (version 3), not psycopg2. Key differences:\n",
    "* `autocommit=True` can be passed as a connection parameter\n",
    "* Better async support\n",
    "* Modern API design\n",
    "\n",
    "**Important**: This uses OAuth tokens that expire after ~1 hour. For production, you'll need to refresh credentials or use native Postgres roles with passwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65017d43-3cfd-434c-8ef4-18ba380366c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Build the LangGraph Agent\n",
    "\n",
    "This creates our conversational agent with:\n",
    "\n",
    "1. **System Prompt**: Guides the agent's behavior and shows current state\n",
    "2. **Agent Node**: Processes user input and generates responses using Claude\n",
    "3. **Information Extraction**: Parses user messages to extract travel preferences\n",
    "4. **Graph Compilation**: Connects everything with checkpointing enabled\n",
    "\n",
    "The agent automatically:\n",
    "* Loads previous conversation state from Lakebase\n",
    "* Updates structured information (destination, dates, etc.)\n",
    "* Saves checkpoints after each turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53057bf-c69f-4c7b-b3d4-2b7498fbfb6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import contextmanager\n",
    "from typing import Generator\n",
    "import psycopg\n",
    "\n",
    "class LakebaseConnectionManager:\n",
    "    \"\"\"Manages connections to Lakebase for checkpoint storage.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str,\n",
    "        database: str,\n",
    "        port: int = 5432,\n",
    "        user: str = None,\n",
    "        password: str = None\n",
    "    ):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self) -> Generator[psycopg.Connection, None, None]:\n",
    "        \"\"\"Get a database connection as a context manager.\"\"\"\n",
    "        conn = psycopg.connect(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            dbname=self.database,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            autocommit=True,\n",
    "            sslmode=\"require\"\n",
    "        )\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "# Initialize with your Lakebase instance details\n",
    "# Generate fresh credentials\n",
    "credential = w.database.generate_database_credential(\n",
    "    instance_names=[LAKEBASE_INSTANCE_NAME]\n",
    ")\n",
    "\n",
    "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "hostname = instance.read_write_dns\n",
    "current_user = w.current_user.me()\n",
    "username = current_user.user_name\n",
    "\n",
    "lakebase_manager = LakebaseConnectionManager(\n",
    "    host=hostname,\n",
    "    database=\"travel_assistant_db\",\n",
    "    user=username,\n",
    "    password=credential.token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e8a33d-fd93-4bcc-b294-cc4610968efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# System prompt that guides the agent's behavior\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful Travel Planning Assistant. Your goal is to help users \n",
    "plan their perfect trip by gathering information about their preferences and providing \n",
    "personalized recommendations.\n",
    "\n",
    "When interacting with users:\n",
    "1. If they haven't specified a destination, ask about it\n",
    "2. Once you know the destination, ask about travel dates if not mentioned\n",
    "3. Gather information about their budget and trip preferences (adventure, relaxation, cultural, etc.)\n",
    "4. Provide helpful suggestions based on what you've learned\n",
    "\n",
    "Always be conversational and remember what the user has already told you. Don't ask for \n",
    "information they've already provided.\n",
    "\n",
    "Current collected information:\n",
    "- Destination: {destination}\n",
    "- Dates: {travel_dates}\n",
    "- Budget: {budget}\n",
    "- Trip type: {trip_type}\n",
    "- Preferences: {preferences}\n",
    "\"\"\"\n",
    "\n",
    "def create_travel_agent_graph(checkpointer: PostgresSaver):\n",
    "    \"\"\"Create the LangGraph agent with checkpointing.\"\"\"\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "    llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "    \n",
    "    def agent_node(state: TravelPlannerState) -> dict:\n",
    "        \"\"\"Main agent node that processes user input and generates responses.\"\"\"\n",
    "        \n",
    "        # Format system prompt with current state\n",
    "        system_content = SYSTEM_PROMPT.format(\n",
    "            destination=state.get(\"destination\", \"Not specified\"),\n",
    "            travel_dates=state.get(\"travel_dates\", \"Not specified\"),\n",
    "            budget=state.get(\"budget\", \"Not specified\"),\n",
    "            trip_type=state.get(\"trip_type\", \"Not specified\"),\n",
    "            preferences=\", \".join(state.get(\"preferences\", [])) or \"None collected\"\n",
    "        )\n",
    "        \n",
    "        # Build messages for the LLM\n",
    "        messages = [SystemMessage(content=system_content)] + state[\"messages\"]\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # Extract any travel information from the conversation\n",
    "        # In a production agent, you might use a separate extraction step\n",
    "        updated_state = extract_travel_info(state, response.content)\n",
    "        updated_state[\"messages\"] = [response]\n",
    "        \n",
    "        return updated_state\n",
    "    \n",
    "    def extract_travel_info(state: TravelPlannerState, response: str) -> dict:\n",
    "        \"\"\"Extract structured travel information from the conversation.\n",
    "        \n",
    "        In a production system, you might use an LLM call for this extraction.\n",
    "        For simplicity, we'll do basic pattern matching here.\n",
    "        \"\"\"\n",
    "        updates = {}\n",
    "        \n",
    "        # Get the last user message for context\n",
    "        user_messages = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)]\n",
    "        if user_messages:\n",
    "            last_user_msg = user_messages[-1].content.lower()\n",
    "            \n",
    "            # Simple extraction logic (production would use LLM)\n",
    "            if not state.get(\"destination\"):\n",
    "                destinations = [\"rome\", \"paris\", \"tokyo\", \"new york\", \"london\", \"barcelona\"]\n",
    "                for dest in destinations:\n",
    "                    if dest in last_user_msg:\n",
    "                        updates[\"destination\"] = dest.title()\n",
    "                        break\n",
    "            \n",
    "            if not state.get(\"budget\"):\n",
    "                if \"budget\" in last_user_msg or \"$\" in last_user_msg:\n",
    "                    # Extract budget mentions\n",
    "                    if \"luxury\" in last_user_msg:\n",
    "                        updates[\"budget\"] = \"Luxury ($500+/day)\"\n",
    "                    elif \"moderate\" in last_user_msg or \"mid\" in last_user_msg:\n",
    "                        updates[\"budget\"] = \"Moderate ($150-300/day)\"\n",
    "                    elif \"budget\" in last_user_msg:\n",
    "                        updates[\"budget\"] = \"Budget ($50-150/day)\"\n",
    "        \n",
    "        return updates\n",
    "    \n",
    "    # Build the graph\n",
    "    graph = StateGraph(TravelPlannerState)\n",
    "    graph.add_node(\"agent\", agent_node)\n",
    "    graph.add_edge(START, \"agent\")\n",
    "    graph.add_edge(\"agent\", END)\n",
    "    \n",
    "    # Compile with checkpointer for state persistence\n",
    "    return graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53eb3b38-a731-4bac-9dc0-bbc4197068c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 9: Run a Multi-Turn Conversation\n",
    "\n",
    "Let's test our agent with a realistic conversation flow. The agent will:\n",
    "\n",
    "1. **Remember context** from previous messages\n",
    "2. **Extract information** (destination, dates, preferences)\n",
    "3. **Avoid repetition** - won't ask for info already provided\n",
    "4. **Persist state** in Lakebase after each turn\n",
    "\n",
    "Each conversation has a unique `thread_id` that groups related messages together. The checkpointer automatically:\n",
    "* Saves state after each turn\n",
    "* Loads previous state before processing new messages\n",
    "* Enables conversation resumption across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8f6fe6-d992-4c02-b117-1e6076225daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 10: Inspect Checkpoint History\n",
    "\n",
    "**Checkpoints** are snapshots of conversation state at specific points in time. They enable:\n",
    "\n",
    "* **Debugging**: See exactly what the agent knew at each step\n",
    "* **Time-travel**: Resume from any previous point\n",
    "* **Branching**: Explore alternative conversation paths\n",
    "\n",
    "Each checkpoint contains:\n",
    "* `checkpoint_id`: Unique identifier\n",
    "* `timestamp`: When it was created\n",
    "* `message_count`: Number of messages so far\n",
    "* `destination`, `travel_dates`: Extracted information\n",
    "\n",
    "Let's view the checkpoint history for our conversation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b64b537-fc80-4742-9f43-0a74f7791138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 11: Branch from a Checkpoint (Time-Travel)\n",
    "\n",
    "One of LangGraph's most powerful features is **checkpoint branching** - the ability to go back to any point in a conversation and explore a different path.\n",
    "\n",
    "**Use cases:**\n",
    "* **A/B testing**: Compare different agent responses\n",
    "* **Error recovery**: Retry from before a mistake\n",
    "* **What-if scenarios**: Explore alternative conversation flows\n",
    "\n",
    "In this example, we'll:\n",
    "1. Find the checkpoint before the user mentioned \"Rome\"\n",
    "2. Branch from that point with \"Paris\" instead\n",
    "3. See how the conversation diverges\n",
    "\n",
    "The original \"Rome\" conversation remains intact in the database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9283ce-05df-4437-969e-cad5303c9a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with thread_id: 0deb3870-d2a4-46cf-b62d-5c4f4801d491\n\nUser: Hi! I'm thinking about planning a trip.\nAgent: Hello! How exciting that you're planning a trip! \uD83C\uDF0D \n\nI'd love to help you plan something amazing. To get started, do you have a destination in mind, or are you still exploring options? If you're open to suggestions, I can help you find the perfect spot based on what you're looking for!\n\nUser: I've always wanted to visit Rome!\nAgent: Rome is an absolutely fantastic choice! \uD83C\uDDEE\uD83C\uDDF9 The Eternal City has so much to offer - incredible history, stunning architecture, world-class art, and of course, amazing food!\n\nTo help me plan the perfect Roman adventure for you, I have a few questions:\n\n**When are you thinking of going?** Do you have specific dates in mind, or are you flexible? (Different seasons offer different experiences - spring and fall are particularly lovely with mild weather and fewer crowds!)\n\nAlso, once we nail down the timing, I'd love to know more about:\n- Your budget range (this helps with accommodation and activity recommendations)\n- What kind of experience you're after - are you interested in ancient history and museums, food and wine experiences, a mix of everything, or something else?\n\nWhat sounds good to you?\n\nUser: I'm thinking early March, maybe the first week. What's the weather like?\nAgent: Early March in Rome is a lovely time to visit! \uD83C\uDF38\n\n**Weather-wise, you can expect:**\n- Temperatures around 10-15°C (50-59°F) during the day\n- Cooler evenings around 5-8°C (41-46°F)\n- Occasional rain, so pack layers and a light rain jacket\n- Fewer tourists than peak season, which means shorter lines at major attractions!\n- Early spring blooms starting to appear\n\nIt's actually a great shoulder season - you'll avoid the summer heat and crowds while still having pleasant weather for exploring.\n\n**Now, let's keep planning:**\n\n1. **What's your budget range for this trip?** (For example: budget-friendly, mid-range, or luxury?) This helps me recommend the right hotels, restaurants, and experiences.\n\n2. **What excites you most about Rome?** Are you:\n   - A history buff eager to explore ancient ruins and the Colosseum?\n   - A foodie wanting to dive into authentic Italian cuisine?\n   - An art lover ready for the Vatican Museums and galleries?\n   - Looking for a romantic getaway?\n   - All of the above?\n\nTell me what speaks to you! \uD83C\uDFDB️\uD83C\uDF5D\n\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def run_conversation_turn(\n",
    "    graph,\n",
    "    user_message: str,\n",
    "    thread_id: str,\n",
    "    lakebase_manager: LakebaseConnectionManager\n",
    ") -> str:\n",
    "    \"\"\"Execute a single turn in the conversation.\"\"\"\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Input for this turn\n",
    "    input_state = {\n",
    "        \"messages\": [HumanMessage(content=user_message)],\n",
    "        \"destination\": None,\n",
    "        \"travel_dates\": None,\n",
    "        \"budget\": None,\n",
    "        \"trip_type\": None,\n",
    "        \"preferences\": []\n",
    "    }\n",
    "    \n",
    "    # Run the graph - checkpointer automatically loads previous state\n",
    "    with lakebase_manager.get_connection() as conn:\n",
    "        checkpointer = PostgresSaver(conn)\n",
    "        \n",
    "        # Ensure tables exist (only needed once)\n",
    "        checkpointer.setup()\n",
    "        \n",
    "        graph = create_travel_agent_graph(checkpointer)\n",
    "        result = graph.invoke(input_state, config)\n",
    "    \n",
    "    # Return the assistant's response\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "# Demonstrate a multi-turn conversation\n",
    "thread_id = str(uuid.uuid4())\n",
    "print(f\"Starting conversation with thread_id: {thread_id}\\n\")\n",
    "\n",
    "# Turn 1\n",
    "response1 = run_conversation_turn(\n",
    "    graph=None,  # Created inside function\n",
    "    user_message=\"Hi! I'm thinking about planning a trip.\",\n",
    "    thread_id=thread_id,\n",
    "    lakebase_manager=lakebase_manager\n",
    ")\n",
    "print(f\"User: Hi! I'm thinking about planning a trip.\")\n",
    "print(f\"Agent: {response1}\\n\")\n",
    "\n",
    "# Turn 2\n",
    "response2 = run_conversation_turn(\n",
    "    graph=None,\n",
    "    user_message=\"I've always wanted to visit Rome!\",\n",
    "    thread_id=thread_id,\n",
    "    lakebase_manager=lakebase_manager\n",
    ")\n",
    "print(f\"User: I've always wanted to visit Rome!\")\n",
    "print(f\"Agent: {response2}\\n\")\n",
    "\n",
    "# Turn 3\n",
    "response3 = run_conversation_turn(\n",
    "    graph=None,\n",
    "    user_message=\"I'm thinking early March, maybe the first week. What's the weather like?\",\n",
    "    thread_id=thread_id,\n",
    "    lakebase_manager=lakebase_manager\n",
    ")\n",
    "print(f\"User: I'm thinking early March, maybe the first week. What's the weather like?\")\n",
    "print(f\"Agent: {response3}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbff1a3a-0568-4c8a-9ef7-4adcc3bd7c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint History:\n--------------------------------------------------------------------------------\n\n[0] Checkpoint: 1f0f2d63-3499-6c...\n    Timestamp: 2026-01-16T12:23:44.587043+00:00\n    Messages: 6\n    Destination: None\n    Preview: Early March in Rome is a lovely time to visit! \uD83C\uDF38\n\n**Weather-wise, you can expect:**\n- Temperatures a...\n\n[1] Checkpoint: 1f0f2d62-f5ec-6b...\n    Timestamp: 2026-01-16T12:23:38.014984+00:00\n    Messages: 5\n    Destination: None\n    Preview: I'm thinking early March, maybe the first week. What's the weather like?\n\n[2] Checkpoint: 1f0f2d62-f5e8-67...\n    Timestamp: 2026-01-16T12:23:38.013256+00:00\n    Messages: 4\n    Destination: Rome\n    Preview: Rome is an absolutely fantastic choice! \uD83C\uDDEE\uD83C\uDDF9 The Eternal City has so much to offer - incredible histor...\n\n[3] Checkpoint: 1f0f2d62-f405-67...\n    Timestamp: 2026-01-16T12:23:37.815409+00:00\n    Messages: 4\n    Destination: Rome\n    Preview: Rome is an absolutely fantastic choice! \uD83C\uDDEE\uD83C\uDDF9 The Eternal City has so much to offer - incredible histor...\n\n[4] Checkpoint: 1f0f2d62-c45d-62...\n    Timestamp: 2026-01-16T12:23:32.818155+00:00\n    Messages: 3\n    Destination: None\n    Preview: I've always wanted to visit Rome!\n\n[5] Checkpoint: 1f0f2d62-c459-65...\n    Timestamp: 2026-01-16T12:23:32.816595+00:00\n    Messages: 2\n    Destination: None\n    Preview: Hello! How exciting that you're planning a trip! \uD83C\uDF0D \n\nI'd love to help you plan something amazing. To...\n\n[6] Checkpoint: 1f0f2d62-c215-69...\n    Timestamp: 2026-01-16T12:23:32.579136+00:00\n    Messages: 2\n    Destination: None\n    Preview: Hello! How exciting that you're planning a trip! \uD83C\uDF0D \n\nI'd love to help you plan something amazing. To...\n\n[7] Checkpoint: 1f0f2d62-a723-69...\n    Timestamp: 2026-01-16T12:23:29.753720+00:00\n    Messages: 1\n    Destination: None\n    Preview: Hi! I'm thinking about planning a trip.\n\n[8] Checkpoint: 1f0f2d62-a71f-67...\n    Timestamp: 2026-01-16T12:23:29.752040+00:00\n    Messages: 0\n    Destination: None\n    Preview: None\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def get_checkpoint_history(\n",
    "    thread_id: str,\n",
    "    lakebase_manager: LakebaseConnectionManager,\n",
    "    limit: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Retrieve checkpoint history for a thread.\n",
    "    \n",
    "    Returns a list of checkpoints with metadata, ordered from most recent to oldest.\n",
    "    \"\"\"\n",
    "    with lakebase_manager.get_connection() as conn:\n",
    "        checkpointer = PostgresSaver(conn)\n",
    "        \n",
    "        # Create a minimal graph just to access state history\n",
    "        graph = StateGraph(TravelPlannerState)\n",
    "        graph.add_node(\"agent\", lambda x: x)\n",
    "        graph.add_edge(START, \"agent\")\n",
    "        graph.add_edge(\"agent\", END)\n",
    "        compiled = graph.compile(checkpointer=checkpointer)\n",
    "        \n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        history = []\n",
    "        for state in compiled.get_state_history(config):\n",
    "            if len(history) >= limit:\n",
    "                break\n",
    "            \n",
    "            # Extract useful information from each checkpoint\n",
    "            messages = state.values.get(\"messages\", [])\n",
    "            history.append({\n",
    "                \"checkpoint_id\": state.config[\"configurable\"][\"checkpoint_id\"],\n",
    "                \"thread_id\": thread_id,\n",
    "                \"timestamp\": state.created_at,\n",
    "                \"next_nodes\": state.next,\n",
    "                \"message_count\": len(messages),\n",
    "                \"last_message\": _get_message_preview(messages),\n",
    "                \"destination\": state.values.get(\"destination\"),\n",
    "                \"travel_dates\": state.values.get(\"travel_dates\")\n",
    "            })\n",
    "        \n",
    "        return history\n",
    "\n",
    "def _get_message_preview(messages: list, max_length: int = 100) -> str:\n",
    "    \"\"\"Get a preview of the last message for checkpoint identification.\"\"\"\n",
    "    if not messages:\n",
    "        return None\n",
    "    last_msg = messages[-1]\n",
    "    content = getattr(last_msg, \"content\", str(last_msg))\n",
    "    return content[:max_length] + \"...\" if len(content) > max_length else content\n",
    "\n",
    "\n",
    "# View the checkpoint history for our conversation\n",
    "history = get_checkpoint_history(thread_id, lakebase_manager, limit=20)\n",
    "\n",
    "print(\"Checkpoint History:\")\n",
    "print(\"-\" * 80)\n",
    "for i, checkpoint in enumerate(history):\n",
    "    print(f\"\\n[{i}] Checkpoint: {checkpoint['checkpoint_id'][:16]}...\")\n",
    "    print(f\"    Timestamp: {checkpoint['timestamp']}\")\n",
    "    print(f\"    Messages: {checkpoint['message_count']}\")\n",
    "    print(f\"    Destination: {checkpoint['destination']}\")\n",
    "    print(f\"    Preview: {checkpoint['last_message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bae6b69-c20f-4307-8a69-688fa58cbfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branching from checkpoint: 1f0f2d62-a71f-67...\nOriginal: User said 'Rome'\nAlternative: User says 'Paris'\n\nUser: I've always wanted to visit Paris!\nAgent: How wonderful! Paris is an absolutely magical city! \uD83D\uDDFC There's so much to see and do - from the iconic Eiffel Tower and the Louvre to charming cafés in Montmartre and strolls along the Seine.\n\nTo help me plan the perfect Parisian adventure for you, when are you thinking of visiting? Do you have specific dates in mind, or are you flexible with timing? \n\nAlso, it would be helpful to know:\n- How long are you planning to stay?\n- Is this your first time in Paris, or have you been before?\n\nThis will help me tailor recommendations that make the most of your time in the City of Light! ✨\n\nNew checkpoint created: 1f0f2d62-a71f-67...\nBranched from: 1f0f2d62-a71f-67...\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def branch_from_checkpoint(\n",
    "    thread_id: str,\n",
    "    checkpoint_id: str,\n",
    "    new_message: str,\n",
    "    lakebase_manager: LakebaseConnectionManager\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Branch from a specific checkpoint with a different message.\n",
    "    \n",
    "    This creates a new fork in the conversation history, preserving the original.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id,\n",
    "            \"checkpoint_id\": checkpoint_id  # Resume from this specific point\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with lakebase_manager.get_connection() as conn:\n",
    "        checkpointer = PostgresSaver(conn)\n",
    "        graph = create_travel_agent_graph(checkpointer)\n",
    "        \n",
    "        # Run with the new message, branching from the checkpoint\n",
    "        input_state = {\n",
    "            \"messages\": [HumanMessage(content=new_message)],\n",
    "            \"destination\": None,\n",
    "            \"travel_dates\": None,\n",
    "            \"budget\": None,\n",
    "            \"trip_type\": None,\n",
    "            \"preferences\": []\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(input_state, config)\n",
    "        \n",
    "        # Get the new checkpoint ID\n",
    "        new_state = graph.get_state(config)\n",
    "        \n",
    "        return {\n",
    "            \"response\": result[\"messages\"][-1].content,\n",
    "            \"new_checkpoint_id\": new_state.config[\"configurable\"][\"checkpoint_id\"],\n",
    "            \"parent_checkpoint_id\": checkpoint_id,\n",
    "            \"destination\": result.get(\"destination\")\n",
    "        }\n",
    "\n",
    "\n",
    "# Find the checkpoint right after the user's greeting (before they mentioned Rome)\n",
    "# This is typically the checkpoint at index 1 or 2 in the history\n",
    "history = get_checkpoint_history(thread_id, lakebase_manager)\n",
    "\n",
    "# Find checkpoint before destination was mentioned\n",
    "branch_point = None\n",
    "for checkpoint in reversed(history):  # Start from oldest\n",
    "    if checkpoint[\"destination\"] is None:\n",
    "        branch_point = checkpoint[\"checkpoint_id\"]\n",
    "        break\n",
    "\n",
    "if branch_point:\n",
    "    print(f\"Branching from checkpoint: {branch_point[:16]}...\")\n",
    "    print(\"Original: User said 'Rome'\")\n",
    "    print(\"Alternative: User says 'Paris'\\n\")\n",
    "    \n",
    "    result = branch_from_checkpoint(\n",
    "        thread_id=thread_id,\n",
    "        checkpoint_id=branch_point,\n",
    "        new_message=\"I've always wanted to visit Paris!\",\n",
    "        lakebase_manager=lakebase_manager\n",
    "    )\n",
    "    \n",
    "    print(f\"User: I've always wanted to visit Paris!\")\n",
    "    print(f\"Agent: {result['response']}\")\n",
    "    print(f\"\\nNew checkpoint created: {result['new_checkpoint_id'][:16]}...\")\n",
    "    print(f\"Branched from: {result['parent_checkpoint_id'][:16]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2259ee-8ce8-4654-a6e6-02fb0c720776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: Production Deployment with MLflow\n",
    "\n",
    "Now let's package our agent for production deployment using **MLflow's ResponsesAgent** framework. This enables:\n",
    "\n",
    "* **Model Serving**: Deploy as a REST API endpoint\n",
    "* **Version Control**: Track agent versions in Unity Catalog\n",
    "* **Resource Management**: Automatic authentication to Databricks resources\n",
    "* **Standardized Interface**: Compatible with Databricks AI Gateway\n",
    "\n",
    "## What is ResponsesAgent?\n",
    "\n",
    "`ResponsesAgent` is MLflow's framework for deploying conversational AI agents. It provides:\n",
    "\n",
    "* **Standard API**: Request/response format compatible with OpenAI's API\n",
    "* **Streaming Support**: Real-time token streaming\n",
    "* **Custom Outputs**: Return metadata like thread_id, checkpoint_id\n",
    "* **Framework Agnostic**: Works with LangGraph, LangChain, or custom code\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "1. **agent.py**: Production-ready agent class with ResponsesAgent interface\n",
    "2. **MLflow Logging**: Package agent with dependencies and resources\n",
    "3. **Unity Catalog**: Register model for governance and versioning\n",
    "4. **Model Serving**: Deploy as a scalable REST endpoint (optional)\n",
    "\n",
    "Let's create the production agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f0a1b7b-2132-4a1f-9fb3-5ae1bb627721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 12: Create the Production Agent File\n",
    "\n",
    "We'll create `agent.py` - a self-contained file that:\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **TravelPlanningAgent Class**: Extends `ResponsesAgent` with:\n",
    "   * `predict()`: Handles synchronous requests\n",
    "   * `predict_stream()`: Handles streaming requests\n",
    "   * Connection management with Lakebase\n",
    "   * Checkpoint history retrieval\n",
    "\n",
    "2. **Environment Variables**: Connection details set at deployment:\n",
    "   * `LAKEBASE_HOST`: Database endpoint\n",
    "   * `LAKEBASE_USER`: OAuth username\n",
    "   * `LAKEBASE_PASSWORD`: OAuth token\n",
    "   * `LAKEBASE_DATABASE`: Database name\n",
    "\n",
    "3. **MLflow Integration**:\n",
    "   * `mlflow.models.set_model(AGENT)`: Registers the agent instance\n",
    "   * Enables MLflow to discover and validate the model\n",
    "\n",
    "**Important**: This file uses `psycopg` (v3), not `psycopg2`, for LangGraph compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b95335-49fa-4661-a16f-24430773925a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 13: Log and Register the Agent in Unity Catalog\n",
    "\n",
    "This final step:\n",
    "\n",
    "1. **Sets environment variables**: So agent.py can connect during validation\n",
    "2. **Defines resources**: Declares the Claude endpoint the agent needs\n",
    "3. **Logs the model**: Packages agent.py with dependencies\n",
    "4. **Registers in Unity Catalog**: Creates version 1 of the model\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "* `name=\"agent\"`: Artifact name within the MLflow run\n",
    "* `python_model=\"agent.py\"`: Path to the agent file\n",
    "* `resources`: Databricks resources for automatic auth passthrough\n",
    "* `pip_requirements`: Pinned versions for reproducibility\n",
    "* `registered_model_name`: Three-level Unity Catalog name\n",
    "\n",
    "**After registration**, you can:\n",
    "* Deploy to Model Serving for REST API access\n",
    "* Query from notebooks using `mlflow.pyfunc.load_model()`\n",
    "* Track versions and lineage in Unity Catalog\n",
    "\n",
    "Let's log the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a4a7b9-4c7d-4fdd-812d-85c01610d0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "# agent.py - Save this as a separate file for MLflow logging\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "from typing import Any, Dict, Generator, List, Optional\n",
    "from contextlib import contextmanager\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "import mlflow\n",
    "import psycopg\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration - these will be set via environment variables at deployment time\n",
    "# For local development, you can set these in your notebook before importing\n",
    "LAKEBASE_HOST = os.environ.get(\"LAKEBASE_HOST\")\n",
    "LAKEBASE_DATABASE = os.environ.get(\"LAKEBASE_DATABASE\", \"travel_assistant_db\")\n",
    "LAKEBASE_USER = os.environ.get(\"LAKEBASE_USER\")\n",
    "LAKEBASE_PASSWORD = os.environ.get(\"LAKEBASE_PASSWORD\")\n",
    "\n",
    "\n",
    "class TravelPlannerState(TypedDict):\n",
    "    \"\"\"State schema for the Travel Planning Assistant.\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    destination: Optional[str]\n",
    "    travel_dates: Optional[str]\n",
    "    budget: Optional[str]\n",
    "    trip_type: Optional[str]\n",
    "    preferences: List[str]\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful Travel Planning Assistant. Your goal is to help users \n",
    "plan their perfect trip by gathering information about their preferences and providing \n",
    "personalized recommendations.\n",
    "\n",
    "When interacting with users:\n",
    "1. If they haven't specified a destination, ask about it\n",
    "2. Once you know the destination, ask about travel dates if not mentioned\n",
    "3. Gather information about their budget and trip preferences\n",
    "4. Provide helpful suggestions based on what you've learned\n",
    "\n",
    "Always be conversational and remember what the user has already told you.\n",
    "\n",
    "Current collected information:\n",
    "- Destination: {destination}\n",
    "- Dates: {travel_dates}\n",
    "- Budget: {budget}\n",
    "- Trip type: {trip_type}\n",
    "- Preferences: {preferences}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TravelPlanningAgent(ResponsesAgent):\n",
    "    \"\"\"Production-ready Travel Planning Agent with Lakebase-backed memory.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Note: Heavy initialization should be deferred to first predict call\n",
    "        to avoid issues in distributed serving environment.\n",
    "        \"\"\"\n",
    "        self._llm = None\n",
    "    \n",
    "    @property\n",
    "    def llm(self):\n",
    "        \"\"\"Lazy initialization of the LLM client.\"\"\"\n",
    "        if self._llm is None:\n",
    "            LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "            self._llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "        return self._llm\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a Lakebase connection as a context manager.\"\"\"\n",
    "        # Validate that connection parameters are set\n",
    "        if not all([LAKEBASE_HOST, LAKEBASE_USER, LAKEBASE_PASSWORD]):\n",
    "            raise ValueError(\n",
    "                \"Lakebase connection parameters not set. \"\n",
    "                \"Please set LAKEBASE_HOST, LAKEBASE_USER, and LAKEBASE_PASSWORD environment variables.\"\n",
    "            )\n",
    "        \n",
    "        conn = psycopg.connect(\n",
    "            host=LAKEBASE_HOST,\n",
    "            port=5432,\n",
    "            dbname=LAKEBASE_DATABASE,\n",
    "            user=LAKEBASE_USER,\n",
    "            password=LAKEBASE_PASSWORD,\n",
    "            sslmode=\"require\",\n",
    "            autocommit=True\n",
    "        )\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def _create_graph(self, checkpointer: PostgresSaver):\n",
    "        \"\"\"Create the LangGraph agent graph.\"\"\"\n",
    "        \n",
    "        def agent_node(state: TravelPlannerState) -> dict:\n",
    "            \"\"\"Process user input and generate response.\"\"\"\n",
    "            system_content = SYSTEM_PROMPT.format(\n",
    "                destination=state.get(\"destination\") or \"Not specified\",\n",
    "                travel_dates=state.get(\"travel_dates\") or \"Not specified\",\n",
    "                budget=state.get(\"budget\") or \"Not specified\",\n",
    "                trip_type=state.get(\"trip_type\") or \"Not specified\",\n",
    "                preferences=\", \".join(state.get(\"preferences\") or []) or \"None\"\n",
    "            )\n",
    "            \n",
    "            messages = [SystemMessage(content=system_content)] + state[\"messages\"]\n",
    "            response = self.llm.invoke(messages)\n",
    "            \n",
    "            return {\"messages\": [response]}\n",
    "        \n",
    "        graph = StateGraph(TravelPlannerState)\n",
    "        graph.add_node(\"agent\", agent_node)\n",
    "        graph.add_edge(START, \"agent\")\n",
    "        graph.add_edge(\"agent\", END)\n",
    "        \n",
    "        return graph.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    def _convert_to_langchain_messages(\n",
    "        self, \n",
    "        messages: List[Dict[str, Any]]\n",
    "    ) -> List[BaseMessage]:\n",
    "        \"\"\"Convert ResponsesAgent messages to LangChain format.\"\"\"\n",
    "        lc_messages = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            \n",
    "            if role == \"user\":\n",
    "                lc_messages.append(HumanMessage(content=content))\n",
    "            elif role == \"assistant\":\n",
    "                lc_messages.append(AIMessage(content=content))\n",
    "            # Add handling for other roles as needed\n",
    "        \n",
    "        return lc_messages\n",
    "    \n",
    "    def _convert_to_response_format(\n",
    "        self,\n",
    "        messages: List[BaseMessage]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert LangChain messages to ResponsesAgent output format.\"\"\"\n",
    "        output = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, AIMessage):\n",
    "                output.append(\n",
    "                    self.create_text_output_item(\n",
    "                        text=msg.content,\n",
    "                        id=str(uuid.uuid4())\n",
    "                    )\n",
    "                )\n",
    "        return output\n",
    "    \n",
    "    def get_checkpoint_history(\n",
    "        self, \n",
    "        thread_id: str, \n",
    "        limit: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve checkpoint history for debugging and time-travel.\"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            checkpointer = PostgresSaver(conn)\n",
    "            graph = self._create_graph(checkpointer)\n",
    "            \n",
    "            history = []\n",
    "            for state in graph.get_state_history(config):\n",
    "                if len(history) >= limit:\n",
    "                    break\n",
    "                \n",
    "                messages = state.values.get(\"messages\", [])\n",
    "                history.append({\n",
    "                    \"checkpoint_id\": state.config[\"configurable\"][\"checkpoint_id\"],\n",
    "                    \"thread_id\": thread_id,\n",
    "                    \"timestamp\": state.created_at,\n",
    "                    \"next_nodes\": state.next,\n",
    "                    \"message_count\": len(messages),\n",
    "                    \"last_message\": self._get_message_preview(messages)\n",
    "                })\n",
    "            \n",
    "            return history\n",
    "    \n",
    "    def _get_message_preview(self, messages: list, max_length: int = 100) -> str:\n",
    "        \"\"\"Get a preview of the last message.\"\"\"\n",
    "        if not messages:\n",
    "            return None\n",
    "        content = getattr(messages[-1], \"content\", \"\")\n",
    "        return content[:max_length] + \"...\" if len(content) > max_length else content\n",
    "    \n",
    "    def predict(\n",
    "        self, \n",
    "        request: ResponsesAgentRequest\n",
    "    ) -> ResponsesAgentResponse:\n",
    "        \"\"\"Handle a prediction request.\n",
    "        \n",
    "        This is the main entry point for non-streaming requests.\n",
    "        \"\"\"\n",
    "        # Extract thread_id from custom_inputs, generate new one if not provided\n",
    "        custom_inputs = dict(request.custom_inputs or {})\n",
    "        if \"thread_id\" not in custom_inputs:\n",
    "            custom_inputs[\"thread_id\"] = str(uuid.uuid4())\n",
    "        \n",
    "        thread_id = custom_inputs[\"thread_id\"]\n",
    "        checkpoint_id = custom_inputs.get(\"checkpoint_id\")  # Optional for branching\n",
    "        \n",
    "        # Build checkpoint configuration\n",
    "        checkpoint_config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        if checkpoint_id:\n",
    "            checkpoint_config[\"configurable\"][\"checkpoint_id\"] = checkpoint_id\n",
    "            logger.info(f\"Branching from checkpoint: {checkpoint_id}\")\n",
    "        \n",
    "        # Convert input messages to LangChain format\n",
    "        lc_messages = self._convert_to_langchain_messages(\n",
    "            [msg.model_dump() for msg in request.input]\n",
    "        )\n",
    "        \n",
    "        # Prepare input state\n",
    "        input_state = {\n",
    "            \"messages\": lc_messages,\n",
    "            \"destination\": None,\n",
    "            \"travel_dates\": None,\n",
    "            \"budget\": None,\n",
    "            \"trip_type\": None,\n",
    "            \"preferences\": []\n",
    "        }\n",
    "        \n",
    "        # Execute the graph\n",
    "        with self.get_connection() as conn:\n",
    "            checkpointer = PostgresSaver(conn)\n",
    "            graph = self._create_graph(checkpointer)\n",
    "            \n",
    "            result = graph.invoke(input_state, checkpoint_config)\n",
    "        \n",
    "        # Convert output to ResponsesAgent format\n",
    "        output = self._convert_to_response_format(result[\"messages\"])\n",
    "        \n",
    "        # Include thread_id and checkpoint info in custom outputs\n",
    "        custom_outputs = {\"thread_id\": thread_id}\n",
    "        \n",
    "        try:\n",
    "            history = self.get_checkpoint_history(thread_id, limit=1)\n",
    "            if history:\n",
    "                custom_outputs[\"checkpoint_id\"] = history[0][\"checkpoint_id\"]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not retrieve checkpoint_id: {e}\")\n",
    "        \n",
    "        if checkpoint_id:\n",
    "            custom_outputs[\"parent_checkpoint_id\"] = checkpoint_id\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=output,\n",
    "            custom_outputs=custom_outputs\n",
    "        )\n",
    "    \n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Handle a streaming prediction request.\n",
    "        \n",
    "        For simplicity, this implementation collects the full response\n",
    "        and streams it. A production implementation would stream tokens.\n",
    "        \"\"\"\n",
    "        # Get the full response\n",
    "        response = self.predict(request)\n",
    "        \n",
    "        # Stream the output items\n",
    "        for item in response.output:\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=item\n",
    "            )\n",
    "\n",
    "\n",
    "# Create the agent instance for MLflow\n",
    "AGENT = TravelPlanningAgent()\n",
    "\n",
    "# Register the model with MLflow (required for code-based logging)\n",
    "mlflow.models.set_model(AGENT)\n",
    "\n",
    "# For local testing\n",
    "if __name__ == \"__main__\":\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    \n",
    "    # Test the agent\n",
    "    test_request = ResponsesAgentRequest(\n",
    "        input=[{\"role\": \"user\", \"content\": \"I want to plan a trip to Rome!\"}]\n",
    "    )\n",
    "    \n",
    "    response = AGENT.predict(test_request)\n",
    "    print(f\"Response: {response.output}\")\n",
    "    print(f\"Thread ID: {response.custom_outputs.get('thread_id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79874148-4446-4fa5-93ca-e122dc1069c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 23"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 View Logged Model at: https://fevm-ay-demo-workspace.cloud.databricks.com/ml/experiments/3859159789242077/models/m-5fb17f88ed744d149f9a1ff2b091fe5a?o=7474653873260502\n2026/01/16 13:52:31 INFO mlflow.pyfunc: Predicting on input example to validate output\nRegistered model 'ankit_yadav.default.travel_planning_agent' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f609b00765343d68fd9816140cf99fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '3' of model 'ankit_yadav.default.travel_planning_agent': https://fevm-ay-demo-workspace.cloud.databricks.com/explore/data/models/ankit_yadav/default/travel_planning_agent/version/3?o=7474653873260502\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged to: models:/m-5fb17f88ed744d149f9a1ff2b091fe5a\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# Set environment variables for agent.py to use\n",
    "# Generate fresh credentials\n",
    "credential = w.database.generate_database_credential(\n",
    "    instance_names=[LAKEBASE_INSTANCE_NAME]\n",
    ")\n",
    "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "current_user = w.current_user.me()\n",
    "\n",
    "os.environ[\"LAKEBASE_HOST\"] = instance.read_write_dns\n",
    "os.environ[\"LAKEBASE_DATABASE\"] = \"travel_assistant_db\"\n",
    "os.environ[\"LAKEBASE_USER\"] = current_user.user_name\n",
    "os.environ[\"LAKEBASE_PASSWORD\"] = credential.token\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(\"/Users/ankit.yadav@databricks.com/travel-planning-agent\")\n",
    "\n",
    "# Define resources the agent needs access to\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-claude-sonnet-4-5\"),\n",
    "]\n",
    "\n",
    "# Log the agent\n",
    "with mlflow.start_run():\n",
    "    logged_agent = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"langgraph-checkpoint-postgres=={get_distribution('langgraph-checkpoint-postgres').version}\",\n",
    "            \"psycopg[binary]\",\n",
    "            f\"langchain=={get_distribution('langchain').version}\",\n",
    "            f\"langchain-core=={get_distribution('langchain-core').version}\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "        ],\n",
    "        registered_model_name=\"ankit_yadav.default.travel_planning_agent\"\n",
    "    )\n",
    "\n",
    "print(f\"Model logged to: {logged_agent.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c90508-1e2f-4f36-89e1-27968027d5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: Production Deployment\n",
    "\n",
    "Now that we have our agent working locally, let's deploy it to production. We'll skip the temporary OAuth token approach and go **straight to production-grade credential management** using:\n",
    "\n",
    "* **Databricks Secrets**: Secure credential storage\n",
    "* **Native PostgreSQL Roles**: Long-lived passwords (no expiration)\n",
    "* **MLflow Model Serving**: Scalable REST API deployment\n",
    "\n",
    "This approach ensures your agent can run indefinitely without token expiration issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf37a9b-641b-4b78-9cdc-e2afe90017fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Production-Ready Credential Management\n",
    "\n",
    "Before deploying to production, let's set up **long-lived credentials** that won't expire:\n",
    "\n",
    "### What We'll Do:\n",
    "\n",
    "1. **Create a Databricks Secret Scope** to store sensitive credentials\n",
    "2. **Create a Native PostgreSQL Role** with password authentication (no expiration)\n",
    "3. **Grant Permissions** to the role for the travel_assistant_db\n",
    "4. **Store Credentials** in Databricks Secrets\n",
    "5. **Update Model Registration** to reference secrets\n",
    "6. **Deploy Endpoint** with secret references\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "* ✅ **No token expiration** - uses password authentication\n",
    "* ✅ **Secure storage** - credentials encrypted in Databricks Secrets\n",
    "* ✅ **Easy rotation** - update secret without redeploying\n",
    "* ✅ **Audit trail** - track who accesses secrets\n",
    "* ✅ **Production-grade** - follows security best practices\n",
    "\n",
    "Let's set it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b65a9c-df55-4055-9efc-ec514a60541e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step A: Create Databricks Secret Scope"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Secret scope already exists: lakebase-prod\n\nSecret scope 'lakebase-prod' is ready for storing credentials\n"
     ]
    }
   ],
   "source": [
    "# Create a secret scope for Lakebase credentials\n",
    "SECRET_SCOPE = \"lakebase-prod\"\n",
    "\n",
    "try:\n",
    "    w.secrets.create_scope(\n",
    "        scope=SECRET_SCOPE\n",
    "    )\n",
    "    print(f\"✓ Created secret scope: {SECRET_SCOPE}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"✓ Secret scope already exists: {SECRET_SCOPE}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"\\nSecret scope '{SECRET_SCOPE}' is ready for storing credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6fd15-7e9c-4a3b-87a7-f3bf2722de40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step A.5: Enable Native PostgreSQL Login"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling native PostgreSQL login on instance: travel-agent-memory\n\nChecking instance state...\n  Current state: DatabaseInstanceState.AVAILABLE\n\n✓ Instance is AVAILABLE\n\n✓ Native PostgreSQL login enabled\n\nThis allows the instance to accept password-based authentication\nin addition to OAuth tokens.\n\nVerification:\n  enable_pg_native_login: True\n\n✓ Native login is now enabled!\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.database import DatabaseInstance, DatabaseInstanceState\n",
    "import time\n",
    "\n",
    "print(f\"Enabling native PostgreSQL login on instance: {LAKEBASE_INSTANCE_NAME}\\n\")\n",
    "\n",
    "# Wait for instance to be AVAILABLE before updating\n",
    "print(\"Checking instance state...\")\n",
    "max_wait = 600  # 10 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < max_wait:\n",
    "    instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "    state = instance.state\n",
    "    \n",
    "    print(f\"  Current state: {state}\")\n",
    "    \n",
    "    if state == DatabaseInstanceState.AVAILABLE:\n",
    "        print(f\"\\n✓ Instance is AVAILABLE\\n\")\n",
    "        break\n",
    "    elif state == DatabaseInstanceState.FAILED:\n",
    "        print(f\"\\n✗ Instance provisioning failed\")\n",
    "        raise Exception(f\"Instance {LAKEBASE_INSTANCE_NAME} is in FAILED state\")\n",
    "    \n",
    "    time.sleep(10)  # Check every 10 seconds\n",
    "else:\n",
    "    print(f\"\\n⚠ Timeout waiting for instance to be AVAILABLE\")\n",
    "    raise Exception(f\"Instance still in {state} state after {max_wait} seconds\")\n",
    "\n",
    "# Update the instance to enable native password authentication\n",
    "w.database.update_database_instance(\n",
    "    name=LAKEBASE_INSTANCE_NAME,\n",
    "    database_instance=DatabaseInstance(\n",
    "        name=LAKEBASE_INSTANCE_NAME,\n",
    "        enable_pg_native_login=True\n",
    "    ),\n",
    "    update_mask=\"enable_pg_native_login\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Native PostgreSQL login enabled\")\n",
    "print(f\"\\nThis allows the instance to accept password-based authentication\")\n",
    "print(f\"in addition to OAuth tokens.\")\n",
    "\n",
    "# Verify the setting\n",
    "time.sleep(2)  # Wait for update to propagate\n",
    "\n",
    "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  enable_pg_native_login: {instance.effective_enable_pg_native_login}\")\n",
    "\n",
    "if instance.effective_enable_pg_native_login:\n",
    "    print(f\"\\n✓ Native login is now enabled!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Setting may take a moment to propagate. Check again in 30 seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d3f2de-9403-4ad8-ab8e-1127427f60cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step B: Create Native PostgreSQL Role"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PostgreSQL role: travel_agent_service\n\n✓ Role already exists: travel_agent_service\n✓ Updated password for: travel_agent_service\n\n✓ PostgreSQL role created successfully\n\nNext: Store credentials in Databricks Secrets\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import secrets\n",
    "import string\n",
    "\n",
    "# Generate a strong password for the service account\n",
    "def generate_password(length=32):\n",
    "    \"\"\"Generate a cryptographically strong password.\"\"\"\n",
    "    alphabet = string.ascii_letters + string.digits + \"!@#$%^&*\"\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "AGENT_USER = \"travel_agent_service\"\n",
    "AGENT_PASSWORD = generate_password()\n",
    "\n",
    "print(f\"Creating PostgreSQL role: {AGENT_USER}\\n\")\n",
    "\n",
    "# Connect as admin using OAuth\n",
    "credential = w.database.generate_database_credential(\n",
    "    instance_names=[LAKEBASE_INSTANCE_NAME]\n",
    ")\n",
    "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "current_user = w.current_user.me()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=instance.read_write_dns,\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=current_user.user_name,\n",
    "    password=credential.token,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the service role\n",
    "try:\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE ROLE {AGENT_USER} WITH LOGIN PASSWORD %s\n",
    "    \"\"\", (AGENT_PASSWORD,))\n",
    "    print(f\"✓ Created role: {AGENT_USER}\")\n",
    "except psycopg2.errors.DuplicateObject:\n",
    "    print(f\"✓ Role already exists: {AGENT_USER}\")\n",
    "    # Update password for existing role\n",
    "    cursor.execute(f\"\"\"\n",
    "        ALTER ROLE {AGENT_USER} WITH PASSWORD %s\n",
    "    \"\"\", (AGENT_PASSWORD,))\n",
    "    print(f\"✓ Updated password for: {AGENT_USER}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n✓ PostgreSQL role created successfully\")\n",
    "print(f\"\\nNext: Store credentials in Databricks Secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de1c9dd-e7a5-4ea3-ba2c-f2bb4c922b78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step C: Grant Database Permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting permissions to travel_agent_service...\n\n✓ Granted CONNECT on travel_assistant_db\n✓ Granted USAGE on schema public\n✓ Granted table permissions\n✓ Granted sequence permissions\n✓ Granted default privileges for future tables\n\n✓ All permissions granted to travel_agent_service\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "print(f\"Granting permissions to {AGENT_USER}...\\n\")\n",
    "\n",
    "# Connect to postgres database as admin\n",
    "credential = w.database.generate_database_credential(\n",
    "    instance_names=[LAKEBASE_INSTANCE_NAME]\n",
    ")\n",
    "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
    "current_user = w.current_user.me()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=instance.read_write_dns,\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=current_user.user_name,\n",
    "    password=credential.token,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Grant database connection permission\n",
    "cursor.execute(f\"\"\"\n",
    "    GRANT CONNECT ON DATABASE travel_assistant_db TO {AGENT_USER}\n",
    "\"\"\")\n",
    "print(f\"✓ Granted CONNECT on travel_assistant_db\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Connect to travel_assistant_db to grant table permissions\n",
    "conn = psycopg2.connect(\n",
    "    host=instance.read_write_dns,\n",
    "    port=5432,\n",
    "    database=\"travel_assistant_db\",\n",
    "    user=current_user.user_name,\n",
    "    password=credential.token,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Grant schema usage\n",
    "cursor.execute(f\"\"\"\n",
    "    GRANT USAGE ON SCHEMA public TO {AGENT_USER}\n",
    "\"\"\")\n",
    "print(f\"✓ Granted USAGE on schema public\")\n",
    "\n",
    "# Grant table permissions (for LangGraph checkpoint tables)\n",
    "cursor.execute(f\"\"\"\n",
    "    GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO {AGENT_USER}\n",
    "\"\"\")\n",
    "print(f\"✓ Granted table permissions\")\n",
    "\n",
    "# Grant sequence permissions (for auto-increment IDs)\n",
    "cursor.execute(f\"\"\"\n",
    "    GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO {AGENT_USER}\n",
    "\"\"\")\n",
    "print(f\"✓ Granted sequence permissions\")\n",
    "\n",
    "# Grant default privileges for future tables\n",
    "cursor.execute(f\"\"\"\n",
    "    ALTER DEFAULT PRIVILEGES IN SCHEMA public \n",
    "    GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO {AGENT_USER}\n",
    "\"\"\")\n",
    "print(f\"✓ Granted default privileges for future tables\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n✓ All permissions granted to {AGENT_USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4703f24-a083-4461-b2f9-c1371a892435",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step D: Store Credentials in Secrets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing credentials in secret scope: lakebase-prod\n\n✓ Stored: lakebase-host\n✓ Stored: lakebase-database\n✓ Stored: lakebase-user\n✓ Stored: lakebase-password\n\n✓ All credentials stored securely in Databricks Secrets\n\nTo reference in endpoint configuration, use:\n  {{secrets/lakebase-prod/lakebase-host}}\n  {{secrets/lakebase-prod/lakebase-database}}\n  {{secrets/lakebase-prod/lakebase-user}}\n  {{secrets/lakebase-prod/lakebase-password}}\n"
     ]
    }
   ],
   "source": [
    "# Store the credentials in Databricks Secrets\n",
    "SECRET_SCOPE = \"lakebase-prod\"\n",
    "\n",
    "print(f\"Storing credentials in secret scope: {SECRET_SCOPE}\\n\")\n",
    "\n",
    "# Store each credential\n",
    "w.secrets.put_secret(\n",
    "    scope=SECRET_SCOPE,\n",
    "    key=\"lakebase-host\",\n",
    "    string_value=instance.read_write_dns\n",
    ")\n",
    "print(f\"✓ Stored: lakebase-host\")\n",
    "\n",
    "w.secrets.put_secret(\n",
    "    scope=SECRET_SCOPE,\n",
    "    key=\"lakebase-database\",\n",
    "    string_value=\"travel_assistant_db\"\n",
    ")\n",
    "print(f\"✓ Stored: lakebase-database\")\n",
    "\n",
    "w.secrets.put_secret(\n",
    "    scope=SECRET_SCOPE,\n",
    "    key=\"lakebase-user\",\n",
    "    string_value=AGENT_USER\n",
    ")\n",
    "print(f\"✓ Stored: lakebase-user\")\n",
    "\n",
    "w.secrets.put_secret(\n",
    "    scope=SECRET_SCOPE,\n",
    "    key=\"lakebase-password\",\n",
    "    string_value=AGENT_PASSWORD\n",
    ")\n",
    "print(f\"✓ Stored: lakebase-password\")\n",
    "\n",
    "print(f\"\\n✓ All credentials stored securely in Databricks Secrets\")\n",
    "print(f\"\\nTo reference in endpoint configuration, use:\")\n",
    "print(f\"  {{{{secrets/{SECRET_SCOPE}/lakebase-host}}}}\")\n",
    "print(f\"  {{{{secrets/{SECRET_SCOPE}/lakebase-database}}}}\")\n",
    "print(f\"  {{{{secrets/{SECRET_SCOPE}/lakebase-user}}}}\")\n",
    "print(f\"  {{{{secrets/{SECRET_SCOPE}/lakebase-password}}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb4bb04-ad08-4b67-8d01-3f29683ee61b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step E: Test the Service Account"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection with service account: travel_agent_service\n\n✓ Successfully connected as: travel_agent_service\n✓ Connected to database: travel_assistant_db\n✓ Can access conversation_memory table (2 rows)\n\n✓ Service account is working correctly!\n\nReady to deploy with production credentials\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "print(f\"Testing connection with service account: {AGENT_USER}\\n\")\n",
    "\n",
    "# Test connection using the service account credentials\n",
    "try:\n",
    "    test_conn = psycopg2.connect(\n",
    "        host=instance.read_write_dns,\n",
    "        port=5432,\n",
    "        database=\"travel_assistant_db\",\n",
    "        user=AGENT_USER,\n",
    "        password=AGENT_PASSWORD,\n",
    "        sslmode=\"require\"\n",
    "    )\n",
    "    \n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute(\"SELECT current_user, current_database()\")\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    print(f\"✓ Successfully connected as: {result[0]}\")\n",
    "    print(f\"✓ Connected to database: {result[1]}\")\n",
    "    \n",
    "    # Test table access\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM conversation_memory\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"✓ Can access conversation_memory table ({count} rows)\")\n",
    "    \n",
    "    cursor.close()\n",
    "    test_conn.close()\n",
    "    \n",
    "    print(f\"\\n✓ Service account is working correctly!\")\n",
    "    print(f\"\\nReady to deploy with production credentials\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"  1. Verify native login is enabled (run cell 37)\")\n",
    "    print(f\"  2. Wait 30 seconds after enabling native login\")\n",
    "    print(f\"  3. Check that role was created successfully\")\n",
    "    print(f\"  4. Verify permissions were granted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c5c06f-f6ca-4d87-b7d1-d298d61554c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step F: Deploy Endpoint with Databricks Secrets\n",
    "\n",
    "Now we'll create/update the serving endpoint to use the secrets we just created. The endpoint will:\n",
    "\n",
    "* Reference secrets using `{{secrets/scope/key}}` syntax\n",
    "* Automatically decrypt secrets at runtime\n",
    "* Never expose credentials in logs or UI\n",
    "* Work indefinitely without token expiration\n",
    "\n",
    "**Note:** If the endpoint already exists, this will update it with the new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24344370-eb93-45ac-91ab-184e798a215d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create/Update Endpoint with Secrets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating production endpoint: travel-planning-agent-prod...\nThis may take 5-10 minutes...\n\n✓ Production endpoint created: travel-planning-agent-prod\n  State: EndpointStateConfigUpdate.NOT_UPDATING\n\n✓ Production endpoint ready with secure credentials!\n\nBenefits:\n  - No token expiration\n  - Credentials stored securely\n  - Easy to rotate (update secret, restart endpoint)\n  - Audit trail for secret access\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedEntityInput,\n",
    "    AiGatewayConfig,\n",
    "    AiGatewayInferenceTableConfig\n",
    ")\n",
    "from databricks.sdk.errors import ResourceAlreadyExists\n",
    "\n",
    "endpoint_name = \"travel-planning-agent-prod\"\n",
    "SECRET_SCOPE = \"lakebase-prod\"\n",
    "\n",
    "print(f\"Creating production endpoint: {endpoint_name}...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=EndpointCoreConfigInput(\n",
    "            name=endpoint_name,\n",
    "            served_entities=[\n",
    "                ServedEntityInput(\n",
    "                    entity_name=\"ankit_yadav.default.travel_planning_agent\",\n",
    "                    entity_version=\"2\",\n",
    "                    scale_to_zero_enabled=True,\n",
    "                    workload_size=\"Small\",\n",
    "                    environment_vars={\n",
    "                        \"LAKEBASE_HOST\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-host}}}}\",\n",
    "                        \"LAKEBASE_DATABASE\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-database}}}}\",\n",
    "                        \"LAKEBASE_USER\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-user}}}}\",\n",
    "                        \"LAKEBASE_PASSWORD\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-password}}}}\"\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        ai_gateway=AiGatewayConfig(\n",
    "            inference_table_config=AiGatewayInferenceTableConfig(\n",
    "                catalog_name=\"ankit_yadav\",\n",
    "                schema_name=\"default\",\n",
    "                table_name_prefix=\"travel_agent_prod\",\n",
    "                enabled=True\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(f\"✓ Production endpoint created: {endpoint_name}\")\n",
    "    print(f\"  State: {endpoint.state.config_update}\")\n",
    "except ResourceAlreadyExists:\n",
    "    print(f\"✓ Endpoint already exists, updating configuration...\")\n",
    "    \n",
    "    # Update existing endpoint\n",
    "    w.serving_endpoints.update_config(\n",
    "        name=endpoint_name,\n",
    "        served_entities=[\n",
    "            ServedEntityInput(\n",
    "                entity_name=\"ankit_yadav.default.travel_planning_agent\",\n",
    "                entity_version=\"2\",\n",
    "                scale_to_zero_enabled=True,\n",
    "                workload_size=\"Small\",\n",
    "                environment_vars={\n",
    "                    \"LAKEBASE_HOST\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-host}}}}\",\n",
    "                    \"LAKEBASE_DATABASE\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-database}}}}\",\n",
    "                    \"LAKEBASE_USER\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-user}}}}\",\n",
    "                    \"LAKEBASE_PASSWORD\": f\"{{{{secrets/{SECRET_SCOPE}/lakebase-password}}}}\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"✓ Endpoint configuration updated\")\n",
    "\n",
    "print(f\"\\n✓ Production endpoint ready with secure credentials!\")\n",
    "print(f\"\\nBenefits:\")\n",
    "print(f\"  - No token expiration\")\n",
    "print(f\"  - Credentials stored securely\")\n",
    "print(f\"  - Easy to rotate (update secret, restart endpoint)\")\n",
    "print(f\"  - Audit trail for secret access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3bc482-b65e-4b96-b174-89631d0de948",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step G: Test Production Endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: EndpointStateReady.READY\n\nTesting production endpoint with persistent memory...\n\n✓ Using notebook authentication token\n\n✓ Started conversation\n  Thread ID: 5fce1f80-bc2d-4ad7-8b79-eea7a1b49624\n  Checkpoint ID: 1f0f3090-de81-6df8-8001-a84238814e79\n\nAgent: Hello! How exciting that you're planning a trip to Japan! \uD83C\uDDEF\uD83C\uDDF5 It's such an amazing destination with incredible culture, food, and experiences.\n\nTo help me create the perfect itinerary for you, I'd love to know a bit more:\n\n**When are you thinking of visiting Japan?** The timing can really shape your experience:\n- Spring (March-May) is famous for cherry blossoms\n- Summer (June-August) has festivals but can be hot and humid\n- Fall (September-November) offers beautiful autumn colors\n- Winter (December-February) is great for skiing and winter illuminations\n\nDo you have specific dates in mind, or are you flexible with timing?\n\n✓ Production endpoint is working!\n\nKey features:\n  - Credentials stored in Databricks Secrets\n  - No token expiration issues\n  - Persistent memory in Lakebase (thread_id: 5fce1f80-bc2d-4ad7-8b79-eea7a1b49624)\n  - Checkpoint system enabled (checkpoint_id: 1f0f3090-de81-6d...)\n  - Ready for production traffic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "endpoint_name = \"travel-planning-agent-prod\"\n",
    "\n",
    "# Get endpoint status\n",
    "endpoint = w.serving_endpoints.get(name=endpoint_name)\n",
    "print(f\"Endpoint Status: {endpoint.state.ready}\\n\")\n",
    "\n",
    "if endpoint.state.ready != EndpointStateReady.READY:\n",
    "    print(\"⚠ Endpoint is not ready yet. Wait a few minutes and try again.\")\n",
    "else:\n",
    "    print(\"Testing production endpoint with persistent memory...\\n\")\n",
    "    \n",
    "    # Get authentication token from notebook context\n",
    "    token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    if not token:\n",
    "        # Get token from notebook context\n",
    "        try:\n",
    "            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "            print(\"✓ Using notebook authentication token\\n\")\n",
    "        except:\n",
    "            print(\"✗ Could not get authentication token\")\n",
    "            print(\"Please set DATABRICKS_TOKEN environment variable or create a PAT\\n\")\n",
    "            token = None\n",
    "    \n",
    "    if token:\n",
    "        # Build request URL and headers\n",
    "        url = f'https://fevm-ay-demo-workspace.cloud.databricks.com/serving-endpoints/{endpoint_name}/invocations'\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        # Make request with correct schema (use 'input' not 'inputs')\n",
    "        request_data = {\n",
    "            \"input\": [{\"role\": \"user\", \"content\": \"Hi! I want to plan a trip to Japan.\"}]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, data=json.dumps(request_data))\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Request failed with status {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            print(f\"\\nTroubleshooting:\")\n",
    "            print(f\"  1. Check endpoint logs in the Serving UI\")\n",
    "            print(f\"  2. Verify secrets are configured correctly\")\n",
    "            print(f\"  3. Ensure native login is enabled on Lakebase instance\")\n",
    "        else:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract data from response\n",
    "            thread_id = response_data.get(\"custom_outputs\", {}).get(\"thread_id\")\n",
    "            checkpoint_id = response_data.get(\"custom_outputs\", {}).get(\"checkpoint_id\")\n",
    "            agent_message = response_data['output'][0]['content'][0]['text']\n",
    "            \n",
    "            print(f\"✓ Started conversation\")\n",
    "            print(f\"  Thread ID: {thread_id}\")\n",
    "            print(f\"  Checkpoint ID: {checkpoint_id}\\n\")\n",
    "            print(f\"Agent: {agent_message}\")\n",
    "            \n",
    "            print(f\"\\n✓ Production endpoint is working!\")\n",
    "            print(f\"\\nKey features:\")\n",
    "            print(f\"  - Credentials stored in Databricks Secrets\")\n",
    "            print(f\"  - No token expiration issues\")\n",
    "            print(f\"  - Persistent memory in Lakebase (thread_id: {thread_id})\")\n",
    "            print(f\"  - Checkpoint system enabled (checkpoint_id: {checkpoint_id[:16]}...)\")\n",
    "            print(f\"  - Ready for production traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f986fb9-8c84-4fee-8bd6-3ef420b4e3f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Conclusion"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## \uD83C\uDF89 Congratulations!\n",
    "\n",
    "You've built a **production-ready AI agent with persistent memory** using Databricks Lakebase and LangGraph!\n",
    "\n",
    "## What We Built\n",
    "\n",
    "### Part 1: Lakebase Database Setup\n",
    "✅ **Managed PostgreSQL Instance**: Zero-ops database with 1 CU  \n",
    "✅ **Custom Database**: `travel_assistant_db` for agent memory  \n",
    "✅ **Conversation Table**: Schema for storing chat history  \n",
    "✅ **Connection Helper**: Reusable function for database access  \n",
    "\n",
    "### Part 2: Stateful Agent with LangGraph\n",
    "✅ **State Schema**: Tracks messages, destination, dates, budget, preferences  \n",
    "✅ **Connection Manager**: psycopg3 integration for checkpointing  \n",
    "✅ **Agent Graph**: Claude-powered conversational agent  \n",
    "✅ **Multi-Turn Conversations**: Context-aware dialogue  \n",
    "✅ **Checkpoint History**: View conversation snapshots  \n",
    "✅ **Time-Travel Branching**: Explore alternative conversation paths  \n",
    "\n",
    "### Part 3: Production Deployment\n",
    "✅ **Native PostgreSQL Role**: `travel_agent_service` with no expiration  \n",
    "✅ **Databricks Secrets**: Secure credential storage in `lakebase-prod` scope  \n",
    "✅ **MLflow Registration**: Version 2 in Unity Catalog  \n",
    "✅ **Production Endpoint**: `travel-planning-agent-prod` with secret references  \n",
    "✅ **Inference Tables**: Automatic logging for monitoring  \n",
    "\n",
    "## Key Technical Learnings\n",
    "\n",
    "* **psycopg3 vs psycopg2**: LangGraph requires psycopg (v3) for PostgresSaver\n",
    "* **Native Login**: Must enable `enable_pg_native_login=True` on Lakebase instance\n",
    "* **Secret Syntax**: Use `{{secrets/scope/key}}` in environment variables\n",
    "* **ResponsesAgent API**: Use `input` (singular), not `inputs` (plural)\n",
    "* **Response Format**: Access `response['output']` (singular), not `outputs`\n",
    "* **Authentication**: Use `dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()` for notebook tokens\n",
    "\n",
    "## Production Architecture\n",
    "\n",
    "```\n",
    "[\uD83D\uDC64 User] \n",
    "    ↓ HTTPS Request\n",
    "[\uD83D\uDD12 Model Serving Endpoint]\n",
    "    ↓ Loads credentials from Databricks Secrets\n",
    "[\uD83E\uDD16 TravelPlanningAgent (ResponsesAgent)]\n",
    "    ↓ Connects with native Postgres role\n",
    "[\uD83D\uDCBE Lakebase PostgreSQL]\n",
    "    ↓ Stores checkpoints\n",
    "[LangGraph PostgresSaver]\n",
    "```\n",
    "\n",
    "## Deployment Workflow Summary\n",
    "\n",
    "1. **Create Lakebase instance** with native login enabled\n",
    "2. **Create native Postgres role** with strong password\n",
    "3. **Grant permissions** to the role\n",
    "4. **Store credentials** in Databricks Secrets\n",
    "5. **Register model** in Unity Catalog with MLflow\n",
    "6. **Deploy endpoint** referencing secrets\n",
    "7. **Query endpoint** with persistent memory\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "### 1. Add Multi-Turn Conversation Support\n",
    "\n",
    "The current test uses a single message. For full conversations:\n",
    "\n",
    "```python\n",
    "# Continue conversation with thread_id\n",
    "request_data = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"I'm thinking March.\"}],\n",
    "    \"custom_inputs\": {\"thread_id\": \"<thread_id_from_previous_response>\"}\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Implement Credential Rotation\n",
    "\n",
    "```python\n",
    "# Update secret with new password\n",
    "w.secrets.put_secret(\n",
    "    scope=\"lakebase-prod\",\n",
    "    key=\"lakebase-password\",\n",
    "    string_value=\"<new_password>\"\n",
    ")\n",
    "\n",
    "# Restart endpoint to pick up new credentials\n",
    "w.serving_endpoints.update_config(name=endpoint_name, ...)\n",
    "```\n",
    "\n",
    "### 3. Monitor with Inference Tables\n",
    "\n",
    "Query the auto-generated inference table:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM ankit_yadav.default.travel_agent_prod_payload\n",
    "ORDER BY request_time DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "### 4. Scale the Lakebase Instance\n",
    "\n",
    "```python\n",
    "w.database.update_database_instance(\n",
    "    name=LAKEBASE_INSTANCE_NAME,\n",
    "    database_instance=DatabaseInstance(\n",
    "        name=LAKEBASE_INSTANCE_NAME,\n",
    "        capacity=\"CU_2\"  # Scale to 2 CUs\n",
    "    ),\n",
    "    update_mask=\"capacity\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Resources\n",
    "\n",
    "* **Lakebase Docs**: [docs.databricks.com/oltp](https://docs.databricks.com/aws/en/oltp/instances/about/)\n",
    "* **LangGraph Docs**: [langchain-ai.github.io/langgraph](https://langchain-ai.github.io/langgraph/)\n",
    "* **MLflow ResponsesAgent**: [mlflow.org/docs/latest/genai/serving/responses-agent](https://mlflow.org/docs/latest/genai/serving/responses-agent/)\n",
    "* **Databricks Secrets**: [docs.databricks.com/security/secrets](https://docs.databricks.com/security/secrets/)\n",
    "* **Model Serving**: [docs.databricks.com/machine-learning/model-serving](https://docs.databricks.com/machine-learning/model-serving/)\n",
    "\n",
    "## Questions?\n",
    "\n",
    "Reach out on LinkedIn or check the Databricks Community forums!\n",
    "\n",
    "Happy building! \uD83D\uDE80"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Lakebase Setup for Travel Assistant Database",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}